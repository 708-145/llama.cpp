
overall task:
Modify the llama-quantize c++ code to read default.smarterquant.json into an internal data structure. Encode each matrix based on the scheme laid out below. Adapt the encoded blocksize in bytes. The json contains the following information for each matrix: Four compression types referring to the block encoding for the first four 256 wide blocks. Each following block is using the fourth mode as well. This is followed by a list of columns into which the original matrix is ordered before applying the block encoding. The inference code gets the same json to be able to decompress the matrices. Modify the inference code as well so that llama-cli and llama-server work with those encoded GGUFs.

hint: compile using these commands:
cmake -B build -DBUILD_SHARED_LIBS=OFF
cmake --build build --config Release -j8 --target llama-quantize     # or llama-cli, etc.

The SmarterQuant feature implementation is partially complete. The following steps are remaining to make it fully functional:

1.  Done
  **Complete Custom Block Quantization Data Packing in `src/llama-quant.cpp` (Step 3 Enhancement):**
    *   **Current State:** The logic identifies which `ggml_type` to use for each 256-column block of a SmarterQuant-enabled tensor and calculates an *approximate* final size. GGUF metadata for block types and permutation is correctly written. Column permutation of `f32_data` is implemented.
    *   **To Do:**
        *   Refactor the quantization part within `llama_model_quantize_impl` (or create a new helper function like `llama_tensor_quantize_smarter_blocks`).
        *   This function must take the (permuted) `f32_data` and the `SmarterQuantTensorInfo` (containing `compression_types`).
        *   It needs to iterate through the tensor's data, likely row by row. For each row:
            *   Process it in 256-element (column) segments.
            *   For segment 0 (columns 0-255), quantize these 256 elements using `compression_types[0]`.
            *   For segment 1 (columns 256-511), quantize using `compression_types[1]`, and so on for segments 2 and 3.
            *   For segment 4 onwards (columns 1024+), quantize using `compression_types[3]`.
            *   This will involve multiple calls to `ggml_quantize_chunk` (or its underlying logic) *for different portions of the same input row*, using different target quantization types.
            *   The quantized data from each block segment must be carefully written into the correct offset in the `new_data` buffer. The `new_data` buffer will hold a mix of differently quantized blocks.
            *   The total `new_size` must be accurately calculated as the sum of the byte sizes of these individually quantized blocks. This is crucial for GGUF correctness.
            *   The `imatrix` (if provided) needs to be correctly indexed and passed for each segment being quantized. If permutation occurred, ensure `imatrix` aligns with the permuted data.

2.  **Done**
  **Implement Custom Dequantization and Unpermutation in `ggml-cpu/ggml-cpu.c` (Step 4 Core Logic):**
    *   **Modifications Made:**
        *   Added `sq_info` pointer to `struct ggml_tensor` in `ggml.h`.
        *   Updated model loading in `src/llama-model.cpp` to populate `ggml_tensor::sq_info`.
        *   Implemented `ggml_get_rows_smarterquant` function in `ggml-cpu/ggml-cpu.c`. This function:
            1.  Checks if a tensor has SmarterQuant info.
            2.  If so, dequantizes row segments based on `sq_info.compression_types` into a temporary buffer.
            3.  Applies inverse column permutation using `sq_info.column_permutation`.
        *   Modified `ggml_compute_forward_get_rows_f32` and `ggml_compute_forward_get_rows_q` in `ggml-cpu/ggml-cpu.c` to call `ggml_get_rows_smarterquant` for SmarterQuant-enabled tensors.
    *   **Remaining for GPU (Future Task):**
        *   GPU backends (CUDA, Metal, SYCL) will need similar modifications to their dequantization kernels (e.g., in `ggml-cuda.cu`, `ggml-metal.m`) to handle mixed block types and unpermutation if dequantization happens directly on the GPU.

3.  **Thorough Testing (Step 6 Enhancement):**
    *   **Current State:** CPU-level numerical correctness tests implemented.
    *   **To Do (after completing 1 & 2 above):**
        *   **Numerical Correctness (CPU Path):**
            *   DONE: Created a test case in `tests/test-smarterquant.cpp` that:
                *   Defines a sample F32 tensor and `SmarterQuantTensorInfo` with mixed quantization types and column permutation.
                *   Quantizes using `llama_tensor_quantize_smarter_blocks`.
                *   Dequantizes using `ggml_get_rows_smarterquant`.
                *   Verifies numerical output against the original F32 data. Test is passing.
            *   DONE: Created small test models and specific `default.smarterquant.json` configs for more comprehensive end-to-end GGUF-based tests in `tests/test-smarterquant-gguf.cpp`.
            *   DONE: Added tests for edge cases (e.g., tensors smaller than a full block segment, different permutations) in `tests/test-smarterquant.cpp`.
        *   **Numerical Correctness (GPU Path - Future Task):**
            *   Implement similar tests for GPU backends once SmarterQuant dequantization is added there.
        *   **GGUF Metadata Round Trip (Optional but good):**
            *   DONE: Tested writing SmarterQuant metadata to GGUF and reading it back correctly as part of the end-to-end GGUF tests in `tests/test-smarterquant-gguf.cpp`.


4.  **Refinements and Optimizations:**
    *   **Performance of Permutation/Unpermutation at Inference:** Perform the matrix multiplication on the permuted data and unpermute the result vector only.
    *   **Memory for Unpermutation:** The current plan (Option A for unpermutation) requires a temporary buffer for the unpermuted F32 row.
        *   **Analysis:** The temporary buffer (`dequantized_permuted_row`) in `ggml_get_rows_smarterquant` is allocated on the stack using `alloca` with a size of `n_cols * sizeof(float)`.
        *   For a tensor with `C` columns, this is `C * 4` bytes.
        *   Example: For `n_cols = 4096`, memory is `16 KB`. For `n_cols = 8192`, memory is `32 KB`.
        *   This memory is allocated per function call and freed upon return.
        *   **Impact:** Generally minor compared to overall model weights, activations, and KV cache sizes.
        *   **Potential Concern:** For extremely large column dimensions (not typical for current LLM weight matrices but possible for other uses), stack allocation via `alloca` could risk stack overflow if stack sizes are limited. However, for common dimensions, it's acceptable.

These steps represent significant work, especially the modifications to `ggml.c` which is performance-critical and central to the library's operations.


background info:
The initial plan consistet of these steps.

Plan Summary:
Explore the codebase: Identify relevant files and current quantization/GGUF handling.
Implement JSON parsing for llama-quantize: Add a JSON library and C++ structures to read default.smarterquant.json.
Modify llama-quantize.cpp for custom encoding: Implement column permutation, block-specific encoding based on JSON, and add new GGUF metadata for custom-encoded tensors.
Modify inference code (llama.cpp, ggml.c) for custom decoding: Read the custom GGUF metadata, use default.smarterquant.json to understand encoding, implement custom dequantization logic that handles per-block types, and apply inverse column permutation.
Adapt llama-cli and llama-server: Ensure they can load and use models with the new custom quantization, making default.smarterquant.json accessible.
Testing: Create test cases, quantize a model, verify GGUF metadata, and compare inference results.
Documentation and Cleanup: Add comments and update documentation.
A key challenge identified is handling potentially different ggml_types for different blocks within a single GGUF tensor, which might require careful GGUF metadata design and modifications to dequantization/computation routines in ggml.c.
