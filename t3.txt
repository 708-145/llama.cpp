Background:
The basic idea of T3 mode is to sort the tensor data by importance based on imatrix values and quantize the important parts with more bits than the less important ones. The tensor data stays in that mixed-quant form during inference.
The input vector is permuted before matmul to match the column order of the tensor. 
The output vector is unpermuted to get the same order as if the multiplication happened with a regular quant.

For Quantization, each tensor is permuted and then split into three subtensors t1, t2, t3 where t1 has a width of 256, t2 of 512 and t3 the rest along the first dimension.
The permutation is specified in a smarterquant.json file as --smarterquant command line parameter to llama-quantize. It is then stored as metadata in the GGUF along with the 3 sub-tensors for each original tensor. Use a suffix of ".t1", ".t2" and ".t3".
The quantization type to use is also specified in that json file. Use the first value for t1, the second value for t2 and the fourth value fot t3. The third value is unused.
Use vendor/nlohmann/ for JSON parsing. Look at the given smarterquant.json to find out format.

compile with 
cmake -B build -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=OFF
cmake --build build --config Release -j8 --target llama-quantize llama-cli

Download Tiny Moe mradermacher/Tiny-Moe-GGUF as example: wget -c https://huggingface.co/mradermacher/Tiny-Moe-GGUF/resolve/main/Tiny-Moe.IQ4_XS.gguf

Inference:
Add t3 Mode in architecture for all models, start with Tiny-Moe as example.
If tensor variants with suffix t1, t2 , t3 exist in the GGUF then use new mode else use existing code. Test this using the regression tests below.
Read permute metadata to permute input and output vector, update model definition in src/llama-model.cpp to permute the inputs of ggml_mul_mat.
architecture change for matmul of input vector inp with the 3 weight tensors w1, w2, w3:
perm_inp = permute(inp)
perm_inp1, perm_inp2, perm_inp3 = split_by_256_512_rest(perm_inp)
perm_out1 = mat_mul (w1, perm_inp1)
perm_out2 = mat_mul (w2, perm_inp2)
perm_out3 = mat_mul (w3, perm_inp3)
perm_out = concat(perm_out1, perm_out2, perm_out3)
outp = unpermute(perm_out)
--
Feature test:
./llama-quantize --allow-requantize --smarterquant Tiny-Moe.smarterquant.json --imatrix Tiny-Moe.imatrix Tiny-Moe.IQ4_XS.gguf Tiny-Moe.IQ4_XS_T3.gguf IQ4_NL
python3 gguf-py/gguf/scripts/gguf_dump.py Tiny-Moe.IQ4_XS.gguf
python3 gguf-py/gguf/scripts/gguf_dump.py Tiny-Moe.IQ4_XS_T3.gguf
./llama-cli -m Tiny-Moe.IQ4_XS_T3.gguf -p "hi" -n 10 --no-mmap 

Regression test with standard quantization: 
./llama-quantize --allow-requantize --imatrix Tiny-Moe.imatrix Tiny-Moe.IQ4_XS.gguf Tiny-Moe.IQ4_NL.gguf IQ4_NL
./llama-cli -m Tiny-Moe.IQ4_NL.gguf -p "hi" -n 10 --no-mmap 

Regression test inference with unmodified GGUF:
./llama-cli -m Tiny-Moe.IQ4_XS.gguf -p "hi" -n 10 --no-mmap 
--
Progress so far:
    I've implemented the T3 quantization feature, which allows for a new, more flexible quantization scheme.

    The changes include:
    - A new `--smarterquant` command-line option for the `quantize` tool.
    - The core T3 quantization logic in `src/llama-quant.cpp`, which reads a JSON file to control the quantization process, permutes tensor columns, splits tensors into three parts, and quantizes each part individually.
    - The T3 tensor loading logic in `src/llama-model-loader.cpp`, which allows the model to load the `.t1`, `.t2`, and `.t3` tensor variants and their permutation data.
    - New `ggml` operations (`ggml_permute_vec` and `ggml_unpermute`) to support the T3 inference logic.
--

